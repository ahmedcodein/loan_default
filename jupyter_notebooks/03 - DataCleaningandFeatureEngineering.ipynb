{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Cleaning and Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Missing data:\n",
        "    * Observer any missing data.\n",
        "    * Clean the dataset.\n",
        "* Perform feature engineering on the datasets\n",
        "* Define a data cleaning and feature engineering pipeline.\n",
        "* Split the data into Train and Test datasets.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* inputs/datasets/loan_data.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Summary and conclusions at the end of each step.\n",
        "* Standard code feature engineering pipeline.\n",
        "* Standard code for data split.\n",
        "* Save the cleaned Train and Test datasets in:\n",
        "  * `[outputs/datasets/collection/cleaned/TestSetCleaned.csv]`.\n",
        "  * `[outputs/datasets/collection/cleaned/TrainSetCleaned.csv]`.\n",
        "* Save the cleaned Train and Test datasets in:\n",
        "  * `[outputs/datasets/collection/feature_engineered/TestSetCleaned.csv]`.\n",
        "  * `[outputs/datasets/collection/feature_engineered/TrainSetCleaned.csv]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "## **Change working directory**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "* Change the working directory from its current folder to its parent folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "* Make the parent of the current directory the new current directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "* Confirm the new current directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Dataset Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/row/LoanDefaultDataset.csv\"))\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Missing Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Observe variable types and any potential transformation are needed. \n",
        "* Observer an missing cells in any of the variables in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
        "if len(vars_with_missing_data) == 0:\n",
        "    print(\"There are no variables with missing data\")\n",
        "else:\n",
        "    print(f\"There are {len(vars_with_missing_data)} in the dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Result:\n",
        "\n",
        "- There are no missing data in any of the variable in the dataset.\n",
        "- No further action is required with regards to Missing Data Step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clean dataset Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Split Train and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "                                        df,\n",
        "                                        df['loan_status'],\n",
        "                                        test_size=0.2,\n",
        "                                        random_state=0)\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save the Output Cleaned Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  os.makedirs(name='outputs/datasets/collection/cleaned') # create outputs/datasets/collection/cleaned folder\n",
        "  TrainSet.to_csv(\"outputs/datasets/collection/cleaned/TrainSet.csv\", index=False)\n",
        "  TestSet.to_csv(\"outputs/datasets/collection/cleaned/TestSet.csv\", index=False)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* A custom function that performs variables analysis and transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from feature_engine import transformation as vt\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "sns.set(style=\"whitegrid\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def calculate_skew_kurtosis(df,col):\n",
        "  print(f\"{col} with skewness: {df[col].skew().round(2)} and kurtosis: {df[col].kurtosis().round(2)}\")\n",
        "\n",
        "def FeatureEngineeringAnalysis(df, analysis_type=None):\n",
        "    \"\"\"\n",
        "    - used for quick feature engineering on numerical and categorical variables\n",
        "    to decide which transformation can better transform the distribution shape\n",
        "    - Once transformed, use a reporting tool, like ydata-profiling, to evaluate distributions\n",
        "    \"\"\"\n",
        "    check_missing_values(df)\n",
        "    allowed_types = ['numerical', 'ordinal_encoder', 'outlier_winsorizer']\n",
        "    check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
        "    list_column_transformers = define_list_column_transformers(analysis_type)\n",
        "\n",
        "    # Loop in each variable and engineer the data according to the analysis type\n",
        "    df_feat_eng = pd.DataFrame([])\n",
        "    for column in df.columns:\n",
        "        # create additional columns (column_method) to apply the methods\n",
        "        df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "        for method in list_column_transformers:\n",
        "            df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "\n",
        "        # Apply transformers in respective column_transformers\n",
        "        df_feat_eng, list_applied_transformers = apply_transformers(\n",
        "            analysis_type, df_feat_eng, column)\n",
        "\n",
        "        # For each variable, assess how the transformations perform\n",
        "        transformer_evaluation(\n",
        "            column, list_applied_transformers, analysis_type, df_feat_eng)\n",
        "\n",
        "    return df_feat_eng\n",
        "\n",
        "\n",
        "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
        "    \"\"\" Check analysis type \"\"\"\n",
        "    if analysis_type is None:\n",
        "        raise SystemExit(\n",
        "            f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
        "    if analysis_type not in allowed_types:\n",
        "        raise SystemExit(\n",
        "            f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
        "\n",
        "\n",
        "def check_missing_values(df):\n",
        "    if df.isna().sum().sum() != 0:\n",
        "        raise SystemExit(\n",
        "            f\"There is a missing value in your dataset. Please handle that before getting into feature engineering.\")\n",
        "\n",
        "\n",
        "def define_list_column_transformers(analysis_type):\n",
        "    \"\"\" Set suffix columns according to analysis_type\"\"\"\n",
        "    if analysis_type == 'numerical':\n",
        "        list_column_transformers = [\n",
        "            \"log_e\", \"log_10\", \"reciprocal\", \"power\", \"box_cox\", \"yeo_johnson\"]\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        list_column_transformers = [\"ordinal_encoder\"]\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        list_column_transformers = ['iqr']\n",
        "\n",
        "    return list_column_transformers\n",
        "\n",
        "\n",
        "def apply_transformers(analysis_type, df_feat_eng, column):\n",
        "    for col in df_feat_eng.select_dtypes(include='category').columns:\n",
        "        df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
        "\n",
        "    if analysis_type == 'numerical':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_Numerical(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_OutlierWinsorizer(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_CategoricalEncoder(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    return df_feat_eng, list_applied_transformers\n",
        "\n",
        "\n",
        "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
        "    # For each variable, assess how the transformations perform\n",
        "    print(f\"* Variable Analyzed: {column}\")\n",
        "    print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
        "    for col in [column] + list_applied_transformers:\n",
        "\n",
        "        if analysis_type != 'ordinal_encoder':\n",
        "            calculate_skew_kurtosis(df_feat_eng,col)\n",
        "            DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        else:\n",
        "            if col == column:\n",
        "                DiagnosticPlots_Categories(df_feat_eng, col)\n",
        "            else:\n",
        "                DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.countplot(data=df_feat_eng, x=col, palette=[\n",
        "                  '#432371'], order=df_feat_eng[col].value_counts().index)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.suptitle(f\"{col}\", fontsize=30, y=1.05)\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    sns.histplot(data=df, x=variable, kde=True, element=\"step\", ax=axes[0])\n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
        "    sns.boxplot(x=df[variable], ax=axes[2])\n",
        "\n",
        "    axes[0].set_title('Histogram')\n",
        "    axes[1].set_title('QQ Plot')\n",
        "    axes[2].set_title('Boxplot')\n",
        "    fig.suptitle(f\"{variable}\", fontsize=30, y=1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def FeatEngineering_CategoricalEncoder(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "    try:\n",
        "        encoder = OrdinalEncoder(encoding_method='arbitrary', variables=[\n",
        "                                 f\"{column}_ordinal_encoder\"])\n",
        "        df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
        "\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_ordinal_encoder\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_OutlierWinsorizer(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # Winsorizer iqr\n",
        "    try:\n",
        "        disc = Winsorizer(\n",
        "            capping_method='iqr', tail='both', fold=1.5, variables=[f\"{column}_iqr\"])\n",
        "        df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_iqr\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_iqr\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_Numerical(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # LogTransformer base e\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_e\"])\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_e\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_e\"], axis=1, inplace=True)\n",
        "\n",
        "    # LogTransformer base 10\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_10\"], base='10')\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_10\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_10\"], axis=1, inplace=True)\n",
        "\n",
        "    # ReciprocalTransformer\n",
        "    try:\n",
        "        rt = vt.ReciprocalTransformer(variables=[f\"{column}_reciprocal\"])\n",
        "        df_feat_eng = rt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_reciprocal\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_reciprocal\"], axis=1, inplace=True)\n",
        "\n",
        "    # PowerTransformer\n",
        "    try:\n",
        "        pt = vt.PowerTransformer(variables=[f\"{column}_power\"])\n",
        "        df_feat_eng = pt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_power\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_power\"], axis=1, inplace=True)\n",
        "\n",
        "    # BoxCoxTransformer\n",
        "    try:\n",
        "        bct = vt.BoxCoxTransformer(variables=[f\"{column}_box_cox\"])\n",
        "        df_feat_eng = bct.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_box_cox\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_box_cox\"], axis=1, inplace=True)\n",
        "\n",
        "    # YeoJohnsonTransformer\n",
        "    try:\n",
        "        yjt = vt.YeoJohnsonTransformer(variables=[f\"{column}_yeo_johnson\"])\n",
        "        df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_yeo_johnson\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Categorical Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Identify categorical variables for the encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_variables_names = df.columns[df.dtypes=='object'].to_list()\n",
        "categorical_variables_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Results:\n",
        "\n",
        "- categorical_variables_names = `['person_gender',\n",
        " 'person_education',\n",
        " 'person_home_ownership',\n",
        " 'loan_intent',\n",
        " 'previous_loan_defaults_on_file']`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Build a DataFrame consisting the selected variables for the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_variables = df[categorical_variables_names].copy()\n",
        "categorical_variables.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Apply the FeatureEngineeringAnalysis function to evaluate the variables transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_variables = FeatureEngineeringAnalysis(df=categorical_variables, analysis_type='ordinal_encoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Results:\n",
        "\n",
        "- Transformed variables with ordinal_encoder  = `['person_gender',\n",
        " 'person_education',\n",
        " 'person_home_ownership',\n",
        " 'loan_intent',\n",
        " 'previous_loan_defaults_on_file']`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Apply the transformation on the categorical_variables using OrdinalEncoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the steps are: \n",
        "# 1 - create a transformer\n",
        "# 2 - fit_transform into df\n",
        "encoder = OrdinalEncoder(encoding_method='arbitrary', variables = categorical_variables_names)\n",
        "df = encoder.fit_transform(df)\n",
        "\n",
        "print(\"* Categorical encoding - ordinal transformation done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Numerical Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Identify numerical variables for potential transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numerical_variables_names = df.columns[(df.dtypes =='float64') | (df.dtypes == 'int64')].to_list()\n",
        "numerical_variables_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Results:\n",
        "\n",
        "- numerical_variables_names = `['person_age',\n",
        " 'person_gender',\n",
        " 'person_education',\n",
        " 'person_income',\n",
        " 'person_emp_exp',\n",
        " 'person_home_ownership',\n",
        " 'loan_amnt',\n",
        " 'loan_intent',\n",
        " 'loan_int_rate',\n",
        " 'loan_percent_income',\n",
        " 'cb_person_cred_hist_length',\n",
        " 'credit_score',\n",
        " 'previous_loan_defaults_on_file',\n",
        " 'loan_status']`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Build a DataFrame consisting the selected variables for the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numerical_variables = df[numerical_variables_names].copy()\n",
        "numerical_variables.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Apply the FeatureEngineeringAnalysis function to evaluate the variables transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numerical_variables = FeatureEngineeringAnalysis(df=numerical_variables, analysis_type='numerical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Summary**\n",
        "\n",
        "From the statistical data and graphs Yeo Johnson transformation proved to be effective on the following variables:\n",
        "\n",
        "- person_income:\n",
        "  - Histogram and QQ plot resemble normal distribution with mild deviation\n",
        "  - The median is in the middle of the Interquartile Range (IQR) of the Boxplot\n",
        "  - Skewness: -0.02\n",
        "  - Kurtosis: 1.01\n",
        "- loan_amount:\n",
        "  - Histogram and QQ plot resemble normal distribution with mild deviation\n",
        "  - The median is in the middle of the Interquartile Range (IQR) of the Boxplot\n",
        "  - Skewness: -0.02\n",
        "  - Kurtosis: -0.38\n",
        "- loan_percent_income:\n",
        "  - Histogram and QQ plot resemble normal distribution with mild deviation\n",
        "  - The median is in the middle of the Interquartile Range (IQR) of the Boxplot\n",
        "  - Skewness: 0.09\n",
        "  - kurtosis: -0.76\n",
        "- credit_score: \n",
        "  - Histogram and QQ plot resemble normal distribution with mild deviation\n",
        "  - The median is in the middle of the Interquartile Range (IQR) of the Boxplot\n",
        "  - Skewness: -0.04\n",
        "  - Kurtosis: -0.32\n",
        "\n",
        "> **Conclusion**:\n",
        "\n",
        "- Yeo Johnson transformation to be applied on the following variables: `['person_income','loan_amnt','loan_percent_income','credit_score']`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_names_for_yeo_johnson= ['person_income','loan_amnt','loan_percent_income','credit_score']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Apply the transformation using Yeo Johnson Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the steps are: \n",
        "# 1 - create a transformer\n",
        "# 2 - fit_transform into df\n",
        "yeo_transformer = vt.YeoJohnsonTransformer(variables = variables_names_for_yeo_johnson)\n",
        "df = yeo_transformer.fit_transform(df)\n",
        "\n",
        "print(\"* Numerical encoding - Yeo Johnson transformation done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SmartCorrelatedSelection Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "smart_correlated_variables = df.copy()\n",
        "smart_correlated_variables.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Apply Smart Correlated Selection to the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.7, selection_method=\"variance\")\n",
        "\n",
        "corr_sel.fit_transform(smart_correlated_variables)\n",
        "corr_sel.correlated_feature_sets_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- List the features to be dropped. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_sel.features_to_drop_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Drop the selected features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from feature_engine.selection import DropFeatures\n",
        "\n",
        "pipeline = Pipeline([\n",
        "      ( 'drop_features', DropFeatures(features_to_drop = corr_sel.features_to_drop_))\n",
        "])\n",
        "\n",
        "df_transformed = pipeline.fit_transform(df)\n",
        "df_transformed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The following variables are to be dropped as per SmartCorrelatedSelection results, these are `['person_age', 'cb_person_cred_hist_length']`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Summary**\n",
        "\n",
        "The following transformations are identified as beneficial for the data pipeline:\n",
        "\n",
        "- Categorical encoding - ordinal transformation on the following variables:\n",
        "   `['person_gender','person_education','person_home_ownership','loan_intent','previous_loan_defaults_on_file']`\n",
        "- Numerical encoding - Yeo Johnson transformation on the following variables:\n",
        "- `['person_income','loan_amnt','loan_percent_income','credit_score]`\n",
        "- SmartCorrelatedSelection to drop variables: `['person_age', 'cb_person_cred_hist_length']`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Engineered Dataset Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Split Train and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "                                        df_transformed,\n",
        "                                        df_transformed['loan_status'],\n",
        "                                        test_size=0.2,\n",
        "                                        random_state=0)\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the Output Feature Engineered Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  os.makedirs(name='outputs/datasets/collection/feature_engineered') # create outputs/datasets/collection/feature_engineered folder\n",
        "  TrainSet.to_csv(\"outputs/datasets/collection/feature_engineered/TrainSet.csv\", index=False)\n",
        "  TestSet.to_csv(\"outputs/datasets/collection/feature_engineered/TestSet.csv\", index=False)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Conclusion**\n",
        "\n",
        "In this notebook a data cleaning, transformation and feature engineering are conducted. The aim is to create two typical **data cleaning and feature engineering datasets** and saved in their respective folders. The typical code to perform the data split and feature engineering are provided below to be used in other notebooks.\n",
        "\n",
        "- The typical **data split code** to be used across the rest of the notebooks of this project is presented below:\n",
        "\n",
        "```\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "                                        df_transformed,\n",
        "                                        df_transformed['loan_status'],\n",
        "                                        test_size=0.2,\n",
        "                                        random_state=0)\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")\n",
        "\n",
        "```\n",
        "- The typical **feature engineering pipeline** pipeline is summarized in the code below:\n",
        "```\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Feature Engineering\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "from feature_engine import transformation as vt\n",
        "\n",
        "\n",
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=[\n",
        "                                                        'person_gender',\n",
        "                                                        'person_education',\n",
        "                                                        'person_home_ownership',\n",
        "                                                        'loan_intent',\n",
        "                                                        'previous_loan_defaults_on_file',\n",
        "                                                        ])),\n",
        "        (\"YeoJohnsonTransformer\", vt.YeoJohnsonTransformer(\n",
        "            variables = [\n",
        "                'person_income',\n",
        "                'loan_amnt',\n",
        "                'loan_percent_income',\n",
        "                'credit_score'\n",
        "                ])),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None,\n",
        "         method=\"spearman\", threshold=0.7, selection_method=\"variance\")), # to be dropped = ['person_age', 'cb_person_cred_hist_length'].\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineDataCleaningAndFeatureEngineering()\n",
        "\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Cluster Analysis with All Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Answer **business requirement 3**: \n",
        "    * The client is interested in identifying any hidden patterns in the data to derive conclusions that can benefit the business. \n",
        "*   Perform Cluster analysis on the dataset to extract typical default profile.\n",
        "*   Evaluate the following hypothesis:\n",
        "    * Default on previous loan(s) makes it unlikely to acquire new loan.\n",
        "    * The higher the loan_percent_income, the higher the probability of rejecting the loan\n",
        "    * However high a person's income, it does not impact loan approval if the loan_to_income ratio is below a defined threshold.\n",
        "    * If debtor has rented home and has no record of default, the likelihood of approving loan is guaranteed.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/row/LoanDefaultDataset.csv\n",
        "* Instructions on which variables to use for data cleaning and feature engineering. Those instructions are found in FeatureEngineering Notebook.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* TBD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **SetUp**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard Library\n",
        "import os\n",
        "import warnings\n",
        "# General Utilities\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "# ML and Data processing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "# Feature Engineering\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from feature_engine import transformation as vt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "### Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "* Change the working directory from its current folder to its parent folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "* Make the parent of the current directory the new current directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "* Confirm the new current directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Dataset Loading**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Load the row dataset.\n",
        "- Drop loan_status since it is not considered a feature for this analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = (pd.read_csv(\n",
        "    \"outputs/datasets/collection/row/LoanDefaultDataset.csv\").drop(['loan_status'], axis=1))\n",
        "print(df.shape)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **ML Cluster Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Define the cluster pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=[\n",
        "                                                         'person_gender',\n",
        "                                                         'person_education',\n",
        "                                                         'person_home_ownership',\n",
        "                                                         'loan_intent',\n",
        "                                                         'previous_loan_defaults_on_file',])),\n",
        "        (\"YeoJohnsonTransformer\", vt.YeoJohnsonTransformer(variables=[\n",
        "            'person_income',\n",
        "            'loan_amnt',\n",
        "            'loan_percent_income',\n",
        "            'credit_score',])),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None,\n",
        "         method=\"spearman\", threshold=0.7, selection_method=\"variance\")), # to be dropped = ['person_age', 'cb_person_cred_hist_length'].\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        (\"PCA\", PCA(n_components=50, random_state=0)),\n",
        "\n",
        "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
        "\n",
        "\n",
        "    ])\n",
        "    return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Apply the cluster pipeline without performing the last two steps, ie. the PCA and the model pipeline fit.\n",
        "- The output dataset will then be analyzed by PCA component analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_pca = Pipeline(pipeline_cluster.steps[:-2])\n",
        "df_pca = pipeline_pca.fit_transform(df)\n",
        "\n",
        "print(df_pca.shape,'\\n', type(df_pca))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Principal Component Analysis (PCA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Perform a PCA analysis on the df_pca dataset to set the optimal number of components for cluster analysis.\n",
        "- This is done by applying the PCA separately to the scaled data, i.e. the output of the pipeline in the step above.\n",
        "- Since the 'loan_status' is dropped while loading the dataset and 'person_age', 'cb_person_cred_hist_length' are dropped by the SmartCorrelatedSelection, the dataset is left with 11 variables (features).\n",
        "- The 11 variables are used for the PCA analysis. ie. n_component = 11."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "n_components = 11\n",
        "\n",
        "\n",
        "def pca_components_analysis(df_pca, n_components):\n",
        "    pca = PCA(n_components=n_components).fit(df_pca)\n",
        "    x_PCA = pca.transform(df_pca)  # array with transformed PCA\n",
        "\n",
        "    ComponentsList = [\"Component \" + str(number)\n",
        "                      for number in range(n_components)]\n",
        "    dfExplVarRatio = pd.DataFrame(\n",
        "        data=np.round(100 * pca.explained_variance_ratio_, 3),\n",
        "        index=ComponentsList,\n",
        "        columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "    dfExplVarRatio['Accumulated Variance'] = dfExplVarRatio['Explained Variance Ratio (%)'].cumsum(\n",
        "    )\n",
        "\n",
        "    PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum(\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "    plt.figure(figsize=(9, 6))\n",
        "    sns.lineplot(data=dfExplVarRatio,  marker=\"o\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(np.arange(0, 110, 10))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "pca_components_analysis(df_pca=df_pca, n_components=n_components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Consider PCA with only 6 components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_components_analysis(df_pca=df_pca,n_components=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Result:\n",
        "\n",
        "* The 6 components explain 70.91% of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Update the cluster pipeline with with 6 components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=[\n",
        "                                                         'person_gender', 'person_education', 'person_home_ownership',\n",
        "                                                         'loan_intent', 'previous_loan_defaults_on_file'])),\n",
        "        (\"YeoJohnsonTransformer\", vt.YeoJohnsonTransformer(variables = ['person_income','loan_amnt',\n",
        "                                                                        'loan_percent_income','credit_score'])),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None,\n",
        "         method=\"spearman\", threshold=0.7, selection_method=\"variance\")),\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        (\"PCA\", PCA(n_components=8, random_state=0)),\n",
        "\n",
        "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
        "\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Elbow Method and Silhouette Score**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Apply Elbow Method and Silhouette Score to identify the optimum number of clusters.\n",
        "- Apply the cluster pipeline up to the PCA step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_analysis = pipeline_analysis.fit_transform(df)\n",
        "\n",
        "print(df_analysis.shape,'\\n', type(df_analysis))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Set the range of the Elbow method is limited to 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rcParams['font.family'] = 'DejaVu Sans'\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11)) # 11 is not inclusive, it will plot until 10\n",
        "visualizer.fit(df_analysis) \n",
        "visualizer.show() \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Result:\n",
        "\n",
        "- The graph suggests a 4 cluster with a sharp drop in the distortion score appears to be between 2 and 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Apply Silhouette analysis to span 2, 3 and 4 cluster groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rcParams['font.family'] = 'DejaVu Sans' # use default font to remove non existence fonts warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 5 is not inclusive, it will stop at 4\n",
        "n_cluster_start, n_cluster_stop = 2, 5\n",
        "\n",
        "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(\n",
        "    n_cluster_start, n_cluster_stop), metric='silhouette')\n",
        "visualizer.fit(df_analysis)\n",
        "visualizer.show()\n",
        "plt.show()\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
        "\n",
        "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
        "    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, random_state=0),\n",
        "                                      colors='yellowbrick')\n",
        "    visualizer.fit(df_analysis)\n",
        "    visualizer.show()\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The best Silhouette Coefficient value appears with 2 cluster.\n",
        "- Therefore, 2 clusters are implemented for this analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=[\n",
        "                                                         'person_gender', 'person_education', 'person_home_ownership',\n",
        "                                                         'loan_intent', 'previous_loan_defaults_on_file'])),\n",
        "        (\"YeoJohnsonTransformer\", vt.YeoJohnsonTransformer(variables = ['person_income','loan_amnt',\n",
        "                                                                        'loan_percent_income','credit_score'])),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None,\n",
        "         method=\"spearman\", threshold=0.7, selection_method=\"variance\")),\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        (\"PCA\", PCA(n_components=8, random_state=0)),\n",
        "\n",
        "        (\"model\", KMeans(n_clusters=2, random_state=0)),\n",
        "\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Display data for training cluster pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.copy()\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Fit the dataset into the cluster pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Add cluster predictions to dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- add \"`Clusters`\" as a new column to the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Display cluster's distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Cluster's distribution is almost balanced with the following distribution: Cluster 0 contribute 51% while Class one contribute to 49%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Visualize PCA components on the clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=df_analysis[:, 0], y=df_analysis[:, 1],\n",
        "                hue=X['Clusters'], palette='Set1', alpha=0.6)\n",
        "plt.scatter(x=pipeline_cluster['model'].cluster_centers_[:, 0], y=pipeline_cluster['model'].cluster_centers_[:, 1],\n",
        "            marker=\"x\", s=169, linewidths=3, color=\"black\")\n",
        "plt.xlabel(\"PCA Component 0\")\n",
        "plt.ylabel(\"PCA Component 1\")\n",
        "plt.title(\"PCA Components colored by Clusters\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Save cluster predictions from this cluster pipeline that considers all variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_predictions_with_all_variables = X['Clusters']\n",
        "cluster_predictions_with_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Fit a classifier for cluster predictions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Copy `X` to a DataFrame `df_clf`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_clf = X.copy()\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Split Train and Test sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_clf.drop(['Clusters'], axis=1),\n",
        "    df_clf['Clusters'],\n",
        "    test_size=0.2,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Classifier Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Create classifier pipeline with **RandomForestClassifier** which has the following configuration: `RandomForestClassifier(\n",
        "            random_state=0,\n",
        "            max_depth=None,\n",
        "            max_leaf_nodes= None,\n",
        "            min_samples_leaf= 1,\n",
        "            min_samples_split= 2,\n",
        "            n_estimators= 140),` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineClf2ExplainClusters():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=[\n",
        "                                                         'person_gender', 'person_education', 'person_home_ownership',\n",
        "                                                         'loan_intent', 'previous_loan_defaults_on_file'])),\n",
        "        (\"YeoJohnsonTransformer\", vt.YeoJohnsonTransformer(variables = ['person_income','loan_amnt',\n",
        "                                                                        'loan_percent_income','credit_score'])),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None,\n",
        "         method=\"spearman\", threshold=0.7, selection_method=\"variance\")),\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        (\"feat_selection\", SelectFromModel(RandomForestClassifier(\n",
        "            random_state=0,\n",
        "            max_depth=None,\n",
        "            max_leaf_nodes= None,\n",
        "            min_samples_leaf= 1,\n",
        "            min_samples_split= 2,\n",
        "            n_estimators= 140))),\n",
        "\n",
        "        (\"model\", RandomForestClassifier(\n",
        "            random_state=0,\n",
        "            max_depth=None,\n",
        "            max_leaf_nodes= None,\n",
        "            min_samples_leaf= 1,\n",
        "            min_samples_split= 2,\n",
        "            n_estimators= 140)),\n",
        "        ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineClf2ExplainClusters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Fit the classifier to the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Evaluate classifier performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Train Set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Test Set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Important Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Assess the most important Features that define a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# after data cleaning and feature engineering, the feature space changes\n",
        "\n",
        "# how many data cleaning and feature engineering steps does your pipeline have?\n",
        "data_cleaning_feat_eng_steps = 3\n",
        "columns_after_data_cleaning_feat_eng = (Pipeline(pipeline_clf_cluster.steps[:data_cleaning_feat_eng_steps])\n",
        "                                        .transform(X_train)\n",
        "                                        .columns)\n",
        "\n",
        "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support(\n",
        ")].to_list()\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()],\n",
        "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "# reassign best features in importance order\n",
        "best_features = df_feature_importance['Feature'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{best_features} \\n\")\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- best_features = `['loan_percent_income', 'loan_int_rate', 'loan_amnt', 'previous_loan_defaults_on_file']`.\n",
        "- Store the best_features \n",
        "- Those features are to be used to explain each cluster's profile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_features_pipeline_all_variables = ['loan_percent_income', 'loan_int_rate', 'loan_amnt', 'previous_loan_defaults_on_file']\n",
        "best_features_pipeline_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Cluster Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Create a DataFrame that contains best features and Clusters Predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_profile = df_clf.copy()\n",
        "df_cluster_profile = df_cluster_profile.filter(items=['loan_percent_income', 'loan_int_rate', 'loan_amnt', 'previous_loan_defaults_on_file']  + ['Clusters'], axis=1)\n",
        "print(df_cluster_profile.shape)\n",
        "df_cluster_profile.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- analyze Loan_status levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_loan_status = pd.read_csv(\"outputs/datasets/collection/row/LoanDefaultDataset.csv\").filter(['loan_status'])\n",
        "df_loan_status['loan_status'] = df_loan_status['loan_status'].astype('object')\n",
        "df_loan_status.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Load function that plots a table with description for all Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def DescriptionAllClusters(df, decimal_points=3):\n",
        "\n",
        "    DescriptionAllClusters = pd.DataFrame(\n",
        "        columns=df.drop(['Clusters'], axis=1).columns)\n",
        "    # iterate on each cluster , calls Clusters_IndividualDescription()\n",
        "    for cluster in df.sort_values(by='Clusters')['Clusters'].unique():\n",
        "\n",
        "        EDA_ClusterSubset = df.query(\n",
        "            f\"Clusters == {cluster}\").drop(['Clusters'], axis=1)\n",
        "        ClusterDescription = Clusters_IndividualDescription(\n",
        "            EDA_ClusterSubset, cluster, decimal_points)\n",
        "        DescriptionAllClusters = DescriptionAllClusters.append(\n",
        "            ClusterDescription)\n",
        "\n",
        "    DescriptionAllClusters.set_index(['Cluster'], inplace=True)\n",
        "    return DescriptionAllClusters\n",
        "\n",
        "\n",
        "def Clusters_IndividualDescription(EDA_Cluster, cluster, decimal_points):\n",
        "\n",
        "    ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
        "    # for a given cluster, iterate over all columns\n",
        "    # if the variable is numerical, calculate the IQR: display as Q1 -- Q3.\n",
        "    # That will show the range for the most common values for the numerical variable\n",
        "    # if the variable is categorical, count the frequencies and displays the top 3 most frequent\n",
        "    # That will show the most common levels for the category\n",
        "\n",
        "    for col in EDA_Cluster.columns:\n",
        "\n",
        "        try:  # eventually a given cluster will have only missing data for a given variable\n",
        "\n",
        "            if EDA_Cluster[col].dtypes == 'object':\n",
        "\n",
        "                top_frequencies = EDA_Cluster.dropna(\n",
        "                    subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
        "                Description = ''\n",
        "\n",
        "                for x in range(len(top_frequencies)):\n",
        "                    freq = top_frequencies.iloc[x]\n",
        "                    category = top_frequencies.index[x][0]\n",
        "                    CategoryPercentage = int(round(freq*100, 0))\n",
        "                    statement = f\"'{category}': {CategoryPercentage}% , \"\n",
        "                    Description = Description + statement\n",
        "\n",
        "                ClustersDescription.at[0, col] = Description[:-2]\n",
        "\n",
        "            elif EDA_Cluster[col].dtypes in ['float', 'int']:\n",
        "                DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
        "                Q1 = round(DescStats.iloc[4, 0], decimal_points)\n",
        "                Q3 = round(DescStats.iloc[6, 0], decimal_points)\n",
        "                Description = f\"{Q1} -- {Q3}\"\n",
        "                ClustersDescription.at[0, col] = Description\n",
        "\n",
        "        except Exception as e:\n",
        "            ClustersDescription.at[0, col] = 'Not available'\n",
        "            print(\n",
        "                f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
        "\n",
        "    ClustersDescription['Cluster'] = str(cluster)\n",
        "\n",
        "    return ClustersDescription"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Display a table to describe cluster profile based on the best features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile = DescriptionAllClusters(df=pd.concat([df_cluster_profile,df_loan_status], axis=1))\n",
        "clusters_profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Result**:\n",
        "* The result does not provide a clear profile description.\n",
        "* In general terms, the following pattern is extracted:\n",
        "  * Debtor is very likely to default (92% of the time) if the loan-to-income ratio is low, interest rate is low (below 11%) and loan amount is below 8000 and the debtor has previously default on loan.\n",
        "  *  Debtor is still somewhat likely to default (62% of the time) if the loan-to-income ratio is high (above 15%), interest rate is low (above 11%) and loan amount is above 8000 and the debtor has previously default on loan.\n",
        "\n",
        "* **Summary**\n",
        "  * It is difficult to extract a clear distinction from those patterns that can help the business.\n",
        "  * Additional Analysis hence is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Load a custom function to plot cluster distribution per Variable (absolute and relative levels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cluster_distribution_per_variable(df, target):\n",
        "    \"\"\"\n",
        "    The data should have 2 variables, the cluster predictions and\n",
        "    the variable you want to analyze with, in this case we call \"target\".\n",
        "    We use plotly express to create 2 plots:\n",
        "    Cluster distribution across the target.\n",
        "    Relative presence of the target level in each cluster.\n",
        "    \"\"\"\n",
        "    df_bar_plot = df.value_counts([\"Clusters\", target]).reset_index()\n",
        "    df_bar_plot.columns = ['Clusters', target, 'Count']\n",
        "    df_bar_plot[target] = df_bar_plot[target].astype('object')\n",
        "\n",
        "    print(f\"Clusters distribution across {target} levels\")\n",
        "    fig = px.bar(df_bar_plot, x='Clusters', y='Count',\n",
        "                 color=target, width=800, height=500)\n",
        "    fig.update_layout(xaxis=dict(tickmode='array',\n",
        "                      tickvals=df['Clusters'].unique()))\n",
        "    fig.show(renderer='jupyterlab')\n",
        "\n",
        "    df_relative = (df\n",
        "                   .groupby([\"Clusters\", target])\n",
        "                   .size()\n",
        "                   .groupby(level=0)\n",
        "                   .apply(lambda x:  100*x / x.sum())\n",
        "                   .reset_index()\n",
        "                   .sort_values(by=['Clusters'])\n",
        "                   )\n",
        "    df_relative.columns = ['Clusters', target, 'Relative Percentage (%)']\n",
        "\n",
        "    print(f\"Relative Percentage (%) of {target} in each cluster\")\n",
        "    fig = px.line(df_relative, x='Clusters', y='Relative Percentage (%)',\n",
        "                  color=target, width=800, height=500)\n",
        "    fig.update_layout(xaxis=dict(tickmode='array',\n",
        "                      tickvals=df['Clusters'].unique()))\n",
        "    fig.update_traces(mode='markers+lines')\n",
        "    fig.show(renderer='jupyterlab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Clusters distribution across loan_status levels & Relative Percentage of loan_status in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_vs_loan_status=  df_loan_status.copy()\n",
        "df_cluster_vs_loan_status['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_loan_status, target='loan_status')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Summary**\n",
        "\n",
        "* It is difficult to extract a clear pattern that can help the business when using all the variables in the dataset.\n",
        "* Hence, additional analysis is required. Hence, the cluster analysis is ought to be repeated by looking at the best features\n",
        "  extracted from the analysis of this Notebook."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
